---
title: "The Data Science Process"
author: "Tony Jiang"
date: "January 12, 2017"
output:
  html_document:
    fig_caption: yes
    highlight: tango
    number_sections: yes
    theme: united
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

This is study notes for "Practical Data Science with R" by Nina Zumel and John Mount, Manning 2014.
- The book: "Practical Data Science with R" by Nina Zumel and John Mount, Manning 2014 (book copyright Manning Publications Co., all rights reserved)
- The support site: GitHub WinVector/zmPDSwR

# The data science process 
##  The roles in a data science project



### Project roles

A few recurring roles in a data science project


Role                 Responsibilites
-----------------    --------------------------------
Project sponsor      Represents the business interest; champions the project
Client               Represents end users' interests; domain expert

##  Stages of a data science project

- Define the goal
- Collect and manage data
- Build the model
- Evaluate and critique model
- Present results and document 
- Deploy the model to solve the problem in the real world
- Define the goal
### Defining the goal
### Data collection and management
### Modeling

### Model evaluation and critique
### Presentation and documentation
### Model deployment and maintenance


##  Setting expectations
### Determining lower and upper boudns on model performance
- The null model: A lowe rbound on performance
- THe Bayes Rate: An upper bound on model performance
## Summary
The data science process involves a lot of back-and-forth between the data scientist and other project stakeholders, and between the different stages of the process. 

# Loading data into R

## Working with data from files
### Workign with well-structured data from files or URLs

```{r}
uciCar <- read.csv("data/car.data.csv",sep=",",header= TRUE)
head(uciCar) #display first 6 rows
class(uciCar)
summary(uciCar)
dim(uciCar)
```

### Using R on less-structured data
- Transforming data in R
you need "schema documentation" or "data dictionary" to decrypt troublesome data. 

Read a data fro mGerman bank credit dataset
```{r}
d <- read.table(paste('http://archive.ics.uci.edu/ml/',
   'machine-learning-databases/statlog/german/german.data',sep=''),
   stringsAsFactors=F,header=F)
print(d[1:3,])
```
Change the column names to something meaningful
```{r}
colnames(d) <- c('Status.of.existing.checking.account',
   'Duration.in.month',  'Credit.history', 'Purpose',
   'Credit.amount', 'Savings account/bonds',
   'Present.employment.since',
   'Installment.rate.in.percentage.of.disposable.income',
   'Personal.status.and.sex', 'Other.debtors/guarantors',
   'Present.residence.since', 'Property', 'Age.in.years',
   'Other.installment.plans', 'Housing',
   'Number.of.existing.credits.at.this.bank', 'Job',
   'Number.of.people.being.liable.to.provide.maintenance.for',
   'Telephone', 'foreign.worker', 'Good.Loan')
d$Good.Loan <- as.factor(ifelse(d$Good.Loan==1,'GoodLoan','BadLoan'))
print(d[1:3,])
```

Building a map to interprest loan use codes
```{r}
mapping <- list(
   'A40'='car (new)',
   'A41'='car (used)',
   'A42'='furniture/equipment',
   'A43'='radio/television',
   'A44'='domestic appliances' # note that other codes are not defiend here. 
   )
```


Transform the data 

```{r}
for(i in 1:(dim(d))[2]) {             	# Note: 1 
   if(class(d[,i])=='character') {
      d[,i] <- as.factor(as.character(mapping[d[,i]]))  	# Note: 2 
   }
}
table(d$Purpose,d$Good.Loan)
```

## Working with relational databases
The right way to work with data found in databases is to connect R directly to the database.

### A production-size example
- United States Census 2011 national PLUMS American Communicty Survey data (www.census.gov/acs/www/data_documentation/pums_data/). 
- Millions of rows
- a few gigabytes when zipped
- This size is the sweet spot for relational datbase or SQL databse. we are not forced to move into a MapReduce or database cluster to do our work.
 
Curating the data
Staging the data into a database
- H2
- SQL Screwdriver
SQuirrel SQL

# Exploring Data
Resist the temptation to dive into the modelig step without looking at the dataset first. 

## Using summary statistics to spot problems
```{r}
custdata<- read.csv('data/custdata.tsv',header=T,sep='\t')

summary(custdata)
```

### Typical problems revealed by data summaries
- Missing values
- Invalid values and outliers
- Data range: relative because of units. (babies age in weeks vs years)
- Units

## Spotting problems using graphics and visualization
```
We cannot expect a small number of numerical values to consistntly convey the wealth of information that exists in data. Numerical reduction methods do not retain the information in the data. ~ William Cleveland, "The Elements of Graphing Data"
```

The use of graphics to examine data is called visualization.

### Visually checking distributions for a single variable
what is the peak value?
How many peaks ?
How normal is the data?
How much does the data vary? 

- Histogram
```{r fig1,fig.cap="A histogram tells you where your data is concentrated"}
library(ggplot2)
ggplot(custdata)+geom_histogram(aes(x=age),binwidth=5,fill="gray")
```

- Density plots

```{r}
library(scales)
ggplot(custdata)+geom_density(aes(x=income))+scale_x_continuous(labels=dollar)
ggplot(custdata)+geom_density(aes(x=income))+scale_x_log10(breaks=c(100,1000,10000,100000),labels=dollar)+annotation_logticks(sides="bt")
```

- Bar charts

```{r}
ggplot(custdata)+geom_bar(aes(x=marital.stat),fill="gray")
```
A horizontal bar chart can be easier to read when there are several categories with long names.
```{r}
ggplot(custdata)+geom_bar(aes(x=state.of.res),fill="gray")+coord_flip()+theme(axis.text.y=element_text(size=rel(0.8)))
```
Cleveland recommends that the data in a bar chart be sorted, to more efficiently extract insight from the data. 
```{r}
statesums<-table(custdata$state.of.res)
statef<-as.data.frame(statesums)
colnames(statef)<-c("state.of.res","count")
statef<-transform(statef,state.of.res=reorder(state.of.res,count))
ggplot(statef)+geom_bar(aes(x=state.of.res,y=count),stat="identity",fill="gray")+coord_flip()+theme(axis.text.y=element_text(size=rel(0.8)))

```

### Visually checking relationships between two variables

- Line Plots

```{r}
x<-runif(100)
y<-x^2+0.2*x
ggplot(data.frame(x=x,y=y),aes(x=x,y=y))+geom_line()+geom_point()
```
- Scatter Plots and smoothing curves
```{r}
custdata2<-subset(custdata,(custdata$age>0 & custdata$age<100 & custdata$income>0))
cor(custdata2$age,custdata2$income)
ggplot(custdata2,aes(x=age,y=income))+geom_point()+ylim(0,200000)
```

Linear fit
```{r}
ggplot(custdata2,aes(x=age,y=income))+geom_point()+geom_smooth(method="lm")+ylim(0,200000)
```

Smoothing Curve
```{r}
ggplot(custdata2,aes(x=age,y=income))+geom_point()+geom_smooth()+ylim(0,200000)
```

Distribution of customers with health insurance, as a function of age

```{r}
ggplot(custdata2,aes(x=age,y=as.numeric(health.ins)))+geom_point(position=position_jitter(w=0.05,h=0.05))+geom_smooth()
```

- Hexbin Plots

```{r}
library(hexbin)
ggplot(custdata2,aes(x=age,y=income))+geom_hex(binwidth=c(5,10000))+geom_smooth(color="white",se=F)+ylim(0,200000)
```

- Bar charts for two categorical variables

Relationship between marital status and the probability of health insurance coverage.
```{r}
# bar chart
ggplot(custdata)+geom_bar(aes(x=marital.stat,fill=health.ins)) 
ggplot(custdata)+geom_bar(aes(x=marital.stat,fill=health.ins),position="dodge")
ggplot(custdata)+geom_bar(aes(x=marital.stat,fill=health.ins),position="fill") 
```

add a rug

```{r}
ggplot(custdata,aes(x=marital.stat))+geom_bar(aes(fill=health.ins),position="fill")+geom_point(aes(y=-0.05),size=0.75,alpha=.3,position=position_jitter(h=0.01))
```

Facetd

```{r}
ggplot(custdata2)+geom_bar(aes(x=housing.type,fill=marital.stat),position="dodge")+theme(axis.text.x=element_text(angle=45,hjust=1))

ggplot(subset(custdata2,housing.type!="NA"))+geom_bar(aes(x=marital.stat,fill=marital.stat),position="dodge")+facet_wrap(~housing.type,scales="free_y")+theme(axis.text.x=element_text(angle=45,hjust=1))
```

## Summary
- Take the time to examine your data before diving into the modeling
- The summary command helps you spot issues with data range, units, data type, and missing or invalid values
- Visualization additionally gives you a sense of data distribution and relationships among variables
- Visualization is an iterative process and helps answer questions about the data. Time spent here is time not wasted during the modeling process.